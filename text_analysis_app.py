import streamlit as st
import pandas as pd
import numpy as np
from janome.tokenizer import Tokenizer
from wordcloud import WordCloud
import matplotlib.pyplot as plt
import japanize_matplotlib
import seaborn as sns
import networkx as nx
import plotly.graph_objects as go
from collections import Counter, defaultdict
from itertools import combinations
import io
from docx import Document
import openpyxl
import re

# ãƒšãƒ¼ã‚¸è¨­å®š
st.set_page_config(
    page_title="ãƒ†ã‚­ã‚¹ãƒˆåˆ†æã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³",
    page_icon="ğŸ“Š",
    layout="wide"
)

# ã‚»ãƒƒã‚·ãƒ§ãƒ³ã‚¹ãƒ†ãƒ¼ãƒˆã®åˆæœŸåŒ–
if 'analyzed_data' not in st.session_state:
    st.session_state.analyzed_data = None
if 'tokens' not in st.session_state:
    st.session_state.tokens = []

class TextAnalyzer:
    def __init__(self):
        self.tokenizer = Tokenizer()
        # ãƒã‚¸ãƒ†ã‚£ãƒ–ãƒ»ãƒã‚¬ãƒ†ã‚£ãƒ–è¾æ›¸ï¼ˆç°¡æ˜“ç‰ˆï¼‰
        self.positive_words = set([
            'è‰¯ã„', 'ã™ã°ã‚‰ã—ã„', 'ç´ æ™´ã‚‰ã—ã„', 'æœ€é«˜', 'å¬‰ã—ã„', 'æ¥½ã—ã„', 'å¹¸ã›', 
            'æº€è¶³', 'å¿«é©', 'ç¾ã—ã„', 'å„ªã‚Œã‚‹', 'ç´ æ•µ', 'æœ€é©', 'åŠ¹æœçš„', 'ä¾¿åˆ©',
            'å¥½ã', 'æ„›', 'æ„Ÿè¬', 'å–œã³', 'æˆåŠŸ', 'é”æˆ', 'ç´ æ™´ã‚‰ã—ã', 'å®‰å¿ƒ',
            'å¿«ã„', 'æ˜ã‚‹ã„', 'æ­£ã—ã„', 'æ–°ã—ã„', 'æ¸…æ½”', 'å®‰å…¨', 'å¥åº·', 'æ´»ç™º'
        ])
        self.negative_words = set([
            'æ‚ªã„', 'ã²ã©ã„', 'é…·ã„', 'æœ€æ‚ª', 'æ‚²ã—ã„', 'è¾›ã„', 'è‹¦ã—ã„', 'ä¸æº€',
            'ä¸å¿«', 'é†œã„', 'åŠ£ã‚‹', 'å«Œã„', 'æ†ã„', 'å¤±æ•—', 'ä¸å®‰', 'å¿ƒé…',
            'æš—ã„', 'æ±šã„', 'å±é™º', 'ç—…æ°—', 'å›°ã‚‹', 'å•é¡Œ', 'é›£ã—ã„', 'é¢å€’',
            'ç–²ã‚Œã‚‹', 'ç—›ã„', 'å¼±ã„', 'ä¸‹æ‰‹', 'æ®‹å¿µ', 'å¾Œæ‚”', 'æ€’ã‚Š', 'æã„'
        ])
        
        # ã‚¹ãƒˆãƒƒãƒ—ãƒ¯ãƒ¼ãƒ‰
        self.stop_words = set([
            'ã™ã‚‹', 'ã‚ã‚‹', 'ã„ã‚‹', 'ãªã‚‹', 'ã‚Œã‚‹', 'ã‚‰ã‚Œã‚‹', 'ã›ã‚‹', 
            'ã•ã›ã‚‹', 'ãã‚Œã‚‹', 'ã‚„ã‚‹', 'ãã ã•ã‚‹', 'ã„ã', 'æ¥ã‚‹',
            'ã“ã¨', 'ã‚‚ã®', 'ã®', 'ã‚“', 'ã“ã‚Œ', 'ãã‚Œ', 'ã‚ã‚Œ', 'ã©ã‚Œ',
            'ãŸã‚', 'ã‚ˆã†', 'ãªã©', 'ã•ã‚“', 'ã¡ã‚ƒã‚“', 'ãã‚“'
        ])
    
    def extract_text_from_docx(self, file):
        """Wordãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚‰ãƒ†ã‚­ã‚¹ãƒˆã‚’æŠ½å‡º"""
        doc = Document(file)
        text = '\n'.join([paragraph.text for paragraph in doc.paragraphs])
        return text
    
    def extract_text_from_excel(self, file):
        """Excelãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚‰ãƒ†ã‚­ã‚¹ãƒˆã‚’æŠ½å‡º"""
        df = pd.read_excel(file)
        # ã™ã¹ã¦ã®åˆ—ã‚’çµåˆ
        text = ' '.join(df.astype(str).values.flatten())
        return text
    
    def extract_text_from_csv(self, file):
        """CSVãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚‰ãƒ†ã‚­ã‚¹ãƒˆã‚’æŠ½å‡º"""
        df = pd.read_csv(file)
        # ã™ã¹ã¦ã®åˆ—ã‚’çµåˆ
        text = ' '.join(df.astype(str).values.flatten())
        return text
    
    def tokenize(self, text, pos_filter=None):
        """å½¢æ…‹ç´ è§£æã‚’å®Ÿæ–½"""
        tokens = []
        for token in self.tokenizer.tokenize(text):
            parts = token.split('\t')
            if len(parts) < 2:
                continue
            
            surface = parts[0]
            features = parts[1].split(',')
            pos = features[0]  # å“è©
            base_form = features[6] if len(features) > 6 else surface
            
            # å“è©ãƒ•ã‚£ãƒ«ã‚¿
            if pos_filter and pos not in pos_filter:
                continue
            
            # ã‚¹ãƒˆãƒƒãƒ—ãƒ¯ãƒ¼ãƒ‰é™¤å»
            if base_form in self.stop_words:
                continue
            
            # 1æ–‡å­—ã®å˜èªã‚’é™¤å¤–
            if len(surface) <= 1:
                continue
            
            tokens.append({
                'surface': surface,
                'pos': pos,
                'base_form': base_form
            })
        
        return tokens
    
    def sentiment_analysis(self, tokens):
        """ã‚»ãƒ³ãƒãƒ¡ãƒ³ãƒˆåˆ†æ"""
        positive_count = 0
        negative_count = 0
        neutral_count = 0
        
        sentiment_words = []
        
        for token in tokens:
            base_form = token['base_form']
            if base_form in self.positive_words:
                positive_count += 1
                sentiment_words.append((base_form, 'positive'))
            elif base_form in self.negative_words:
                negative_count += 1
                sentiment_words.append((base_form, 'negative'))
            else:
                neutral_count += 1
        
        total = positive_count + negative_count + neutral_count
        
        if total == 0:
            return {
                'positive': 0,
                'negative': 0,
                'neutral': 0,
                'score': 0,
                'sentiment_words': []
            }
        
        # ã‚»ãƒ³ãƒãƒ¡ãƒ³ãƒˆã‚¹ã‚³ã‚¢ï¼ˆ-1ã‹ã‚‰1ã®ç¯„å›²ï¼‰
        score = (positive_count - negative_count) / total if total > 0 else 0
        
        return {
            'positive': positive_count,
            'negative': negative_count,
            'neutral': neutral_count,
            'positive_ratio': positive_count / total,
            'negative_ratio': negative_count / total,
            'neutral_ratio': neutral_count / total,
            'score': score,
            'sentiment_words': sentiment_words
        }
    
    def create_wordcloud(self, tokens, max_words=100):
        """Wordcloudã‚’ç”Ÿæˆ"""
        # åŸºæœ¬å½¢ã§é›†è¨ˆ
        words = [token['base_form'] for token in tokens]
        text = ' '.join(words)
        
        if not text.strip():
            return None
        
        # æ—¥æœ¬èªãƒ•ã‚©ãƒ³ãƒˆã®ãƒ‘ã‚¹
        font_path = '/usr/share/fonts/truetype/droid/DroidSansFallbackFull.ttf'
        try:
            wordcloud = WordCloud(
                font_path=font_path,
                width=800,
                height=400,
                background_color='white',
                max_words=max_words,
                colormap='viridis'
            ).generate(text)
        except Exception as e:
            # ãƒ•ã‚©ãƒ³ãƒˆãŒè¦‹ã¤ã‹ã‚‰ãªã„å ´åˆã®ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯
            st.warning(f"æ—¥æœ¬èªãƒ•ã‚©ãƒ³ãƒˆã§ã®ç”Ÿæˆã«å¤±æ•—ã—ã¾ã—ãŸ: {e}")
            wordcloud = WordCloud(
                width=800,
                height=400,
                background_color='white',
                max_words=max_words,
                colormap='viridis'
            ).generate(text)
        
        return wordcloud
    
    def cooccurrence_network(self, text, window_size=5, min_count=2):
        """å…±èµ·ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯åˆ†æ"""
        # æ–‡ç« ã‚’æ–‡ã«åˆ†å‰²
        sentences = re.split('[ã€‚ï¼\n]', text)
        
        # å…±èµ·ã‚«ã‚¦ãƒ³ãƒˆ
        cooccurrence = defaultdict(int)
        word_count = defaultdict(int)
        
        for sentence in sentences:
            if not sentence.strip():
                continue
            
            tokens = self.tokenize(sentence, pos_filter=['åè©', 'å‹•è©', 'å½¢å®¹è©'])
            words = [token['base_form'] for token in tokens]
            
            # å˜èªã‚«ã‚¦ãƒ³ãƒˆ
            for word in words:
                word_count[word] += 1
            
            # ã‚¦ã‚£ãƒ³ãƒ‰ã‚¦å†…ã®å…±èµ·ã‚’ã‚«ã‚¦ãƒ³ãƒˆ
            for i, word1 in enumerate(words):
                start = max(0, i - window_size)
                end = min(len(words), i + window_size + 1)
                for j in range(start, end):
                    if i != j:
                        word2 = words[j]
                        pair = tuple(sorted([word1, word2]))
                        cooccurrence[pair] += 1
        
        # æœ€å°å‡ºç¾å›æ•°ã§ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°
        filtered_cooccurrence = {
            pair: count for pair, count in cooccurrence.items()
            if count >= min_count and 
            word_count[pair[0]] >= min_count and 
            word_count[pair[1]] >= min_count
        }
        
        return filtered_cooccurrence, word_count
    
    def create_network_graph(self, cooccurrence, word_count, top_n=30):
        """ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã‚°ãƒ©ãƒ•ã‚’ä½œæˆ"""
        if not cooccurrence:
            return None
        
        # ã‚°ãƒ©ãƒ•ã®ä½œæˆ
        G = nx.Graph()
        
        # ã‚¨ãƒƒã‚¸ã‚’è¿½åŠ ï¼ˆé‡ã¿ã¯å…±èµ·å›æ•°ï¼‰
        for (word1, word2), count in sorted(cooccurrence.items(), key=lambda x: x[1], reverse=True)[:top_n]:
            G.add_edge(word1, word2, weight=count)
        
        if len(G.nodes()) == 0:
            return None
        
        # ãƒ¬ã‚¤ã‚¢ã‚¦ãƒˆã®è¨ˆç®—
        pos = nx.spring_layout(G, k=0.5, iterations=50)
        
        # ã‚¨ãƒƒã‚¸ã®æç”»
        edge_trace = []
        for edge in G.edges():
            x0, y0 = pos[edge[0]]
            x1, y1 = pos[edge[1]]
            weight = G[edge[0]][edge[1]]['weight']
            edge_trace.append(
                go.Scatter(
                    x=[x0, x1, None],
                    y=[y0, y1, None],
                    mode='lines',
                    line=dict(width=weight * 0.5, color='#888'),
                    hoverinfo='none',
                    showlegend=False
                )
            )
        
        # ãƒãƒ¼ãƒ‰ã®æç”»
        node_x = []
        node_y = []
        node_text = []
        node_size = []
        
        for node in G.nodes():
            x, y = pos[node]
            node_x.append(x)
            node_y.append(y)
            node_text.append(f'{node}<br>å‡ºç¾å›æ•°: {word_count[node]}')
            node_size.append(word_count[node] * 2)
        
        node_trace = go.Scatter(
            x=node_x,
            y=node_y,
            mode='markers+text',
            text=[node for node in G.nodes()],
            textposition='top center',
            hovertext=node_text,
            hoverinfo='text',
            marker=dict(
                size=node_size,
                color='lightblue',
                line=dict(width=2, color='darkblue')
            ),
            showlegend=False
        )
        
        # å›³ã®ä½œæˆ
        fig = go.Figure(data=edge_trace + [node_trace],
                       layout=go.Layout(
                           title='å…±èµ·ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯',
                           showlegend=False,
                           hovermode='closest',
                           margin=dict(b=0, l=0, r=0, t=40),
                           xaxis=dict(showgrid=False, zeroline=False, showticklabels=False),
                           yaxis=dict(showgrid=False, zeroline=False, showticklabels=False),
                           height=600
                       ))
        
        return fig


def main():
    st.title("ğŸ“Š ãƒ†ã‚­ã‚¹ãƒˆåˆ†æã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³")
    st.markdown("""
    ã“ã®ã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³ã§ã¯ã€Wordã€Excelã€CSVãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚‰æ–‡ç« ãƒ‡ãƒ¼ã‚¿ã‚’èª­ã¿è¾¼ã¿ã€
    ä»¥ä¸‹ã®åˆ†æã‚’å®Ÿæ–½ã—ã¾ã™ï¼š
    - ğŸ“ å½¢æ…‹ç´ è§£æ
    - ğŸ’­ ã‚»ãƒ³ãƒãƒ¡ãƒ³ãƒˆåˆ†æ
    - â˜ï¸ Wordcloudç”Ÿæˆ
    - ğŸ•¸ï¸ å…±èµ·ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯åˆ†æ
    """)
    
    # ã‚µã‚¤ãƒ‰ãƒãƒ¼
    st.sidebar.header("è¨­å®š")
    
    # ãƒ•ã‚¡ã‚¤ãƒ«ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰
    uploaded_file = st.sidebar.file_uploader(
        "ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰",
        type=['docx', 'xlsx', 'csv', 'txt']
    )
    
    analyzer = TextAnalyzer()
    
    if uploaded_file is not None:
        # ãƒ•ã‚¡ã‚¤ãƒ«ã‚¿ã‚¤ãƒ—ã«å¿œã˜ã¦ãƒ†ã‚­ã‚¹ãƒˆã‚’æŠ½å‡º
        try:
            if uploaded_file.name.endswith('.docx'):
                text = analyzer.extract_text_from_docx(uploaded_file)
            elif uploaded_file.name.endswith('.xlsx'):
                text = analyzer.extract_text_from_excel(uploaded_file)
            elif uploaded_file.name.endswith('.csv'):
                text = analyzer.extract_text_from_csv(uploaded_file)
            else:  # txt
                text = uploaded_file.read().decode('utf-8')
            
            st.success(f"âœ… ãƒ•ã‚¡ã‚¤ãƒ« '{uploaded_file.name}' ã‚’èª­ã¿è¾¼ã¿ã¾ã—ãŸ")
            
            # ãƒ†ã‚­ã‚¹ãƒˆãƒ—ãƒ¬ãƒ“ãƒ¥ãƒ¼
            with st.expander("ğŸ“„ ãƒ†ã‚­ã‚¹ãƒˆãƒ—ãƒ¬ãƒ“ãƒ¥ãƒ¼"):
                st.text_area("èª­ã¿è¾¼ã‚“ã ãƒ†ã‚­ã‚¹ãƒˆ", text[:1000] + "..." if len(text) > 1000 else text, height=200)
            
            # åˆ†æå®Ÿè¡Œãƒœã‚¿ãƒ³
            if st.sidebar.button("ğŸ” åˆ†æã‚’å®Ÿè¡Œ", type="primary"):
                with st.spinner("åˆ†æä¸­..."):
                    # å½¢æ…‹ç´ è§£æ
                    tokens = analyzer.tokenize(text, pos_filter=['åè©', 'å‹•è©', 'å½¢å®¹è©', 'å‰¯è©'])
                    st.session_state.tokens = tokens
                    
                    # åŸºæœ¬çµ±è¨ˆ
                    st.header("ğŸ“Š åŸºæœ¬çµ±è¨ˆ")
                    col1, col2, col3, col4 = st.columns(4)
                    
                    with col1:
                        st.metric("ç·æ–‡å­—æ•°", len(text))
                    with col2:
                        st.metric("ç·å˜èªæ•°", len(tokens))
                    with col3:
                        unique_words = len(set([t['base_form'] for t in tokens]))
                        st.metric("ãƒ¦ãƒ‹ãƒ¼ã‚¯å˜èªæ•°", unique_words)
                    with col4:
                        if len(tokens) > 0:
                            st.metric("èªå½™ã®è±Šã‹ã•", f"{unique_words / len(tokens):.2%}")
                        else:
                            st.metric("èªå½™ã®è±Šã‹ã•", "N/A")
                    
                    # é »å‡ºå˜èª
                    st.header("ğŸ“ˆ é »å‡ºå˜èª")
                    word_freq = Counter([token['base_form'] for token in tokens])
                    top_words = word_freq.most_common(20)
                    
                    if top_words:
                        df_freq = pd.DataFrame(top_words, columns=['å˜èª', 'å‡ºç¾å›æ•°'])
                        
                        col1, col2 = st.columns([1, 1])
                        
                        with col1:
                            st.dataframe(df_freq, use_container_width=True)
                        
                        with col2:
                            fig, ax = plt.subplots(figsize=(10, 6))
                            ax.barh(range(len(top_words)), [count for _, count in top_words])
                            ax.set_yticks(range(len(top_words)))
                            ax.set_yticklabels([word for word, _ in top_words])
                            ax.invert_yaxis()
                            ax.set_xlabel('å‡ºç¾å›æ•°')
                            ax.set_title('é »å‡ºå˜èªãƒˆãƒƒãƒ—20')
                            st.pyplot(fig)
                    
                    # ã‚»ãƒ³ãƒãƒ¡ãƒ³ãƒˆåˆ†æ
                    st.header("ğŸ’­ ã‚»ãƒ³ãƒãƒ¡ãƒ³ãƒˆåˆ†æ")
                    sentiment_result = analyzer.sentiment_analysis(tokens)
                    
                    col1, col2, col3 = st.columns(3)
                    
                    with col1:
                        st.metric("ãƒã‚¸ãƒ†ã‚£ãƒ–å˜èª", sentiment_result['positive'])
                    with col2:
                        st.metric("ãƒã‚¬ãƒ†ã‚£ãƒ–å˜èª", sentiment_result['negative'])
                    with col3:
                        st.metric("ã‚»ãƒ³ãƒãƒ¡ãƒ³ãƒˆã‚¹ã‚³ã‚¢", f"{sentiment_result['score']:.3f}")
                    
                    # ã‚»ãƒ³ãƒãƒ¡ãƒ³ãƒˆæ¯”ç‡ã®å††ã‚°ãƒ©ãƒ•
                    if sentiment_result['positive'] + sentiment_result['negative'] + sentiment_result['neutral'] > 0:
                        fig, ax = plt.subplots(figsize=(8, 6))
                        sizes = [
                            sentiment_result['positive'],
                            sentiment_result['negative'],
                            sentiment_result['neutral']
                        ]
                        labels = ['ãƒã‚¸ãƒ†ã‚£ãƒ–', 'ãƒã‚¬ãƒ†ã‚£ãƒ–', 'ãƒ‹ãƒ¥ãƒ¼ãƒˆãƒ©ãƒ«']
                        colors = ['#90EE90', '#FFB6C6', '#D3D3D3']
                        ax.pie(sizes, labels=labels, colors=colors, autopct='%1.1f%%', startangle=90)
                        ax.set_title('æ„Ÿæƒ…ã®åˆ†å¸ƒ')
                        st.pyplot(fig)
                    
                    # æ„Ÿæƒ…ã‚’æŒã¤å˜èªã®ãƒªã‚¹ãƒˆ
                    if sentiment_result['sentiment_words']:
                        with st.expander("æ„Ÿæƒ…ã‚’æŒã¤å˜èªã®è©³ç´°"):
                            positive_words = [word for word, sentiment in sentiment_result['sentiment_words'] if sentiment == 'positive']
                            negative_words = [word for word, sentiment in sentiment_result['sentiment_words'] if sentiment == 'negative']
                            
                            col1, col2 = st.columns(2)
                            with col1:
                                st.write("**ãƒã‚¸ãƒ†ã‚£ãƒ–å˜èª:**")
                                st.write(", ".join(set(positive_words)))
                            with col2:
                                st.write("**ãƒã‚¬ãƒ†ã‚£ãƒ–å˜èª:**")
                                st.write(", ".join(set(negative_words)))
                    
                    # Wordcloud
                    st.header("â˜ï¸ Wordcloud")
                    max_words = st.sidebar.slider("æœ€å¤§è¡¨ç¤ºå˜èªæ•°", 50, 200, 100)
                    
                    wordcloud = analyzer.create_wordcloud(tokens, max_words=max_words)
                    if wordcloud:
                        fig, ax = plt.subplots(figsize=(12, 6))
                        ax.imshow(wordcloud, interpolation='bilinear')
                        ax.axis('off')
                        st.pyplot(fig)
                    else:
                        st.warning("Wordcloudã‚’ç”Ÿæˆã§ãã¾ã›ã‚“ã§ã—ãŸã€‚")
                    
                    # å…±èµ·ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯
                    st.header("ğŸ•¸ï¸ å…±èµ·ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯åˆ†æ")
                    
                    window_size = st.sidebar.slider("å…±èµ·ã‚¦ã‚£ãƒ³ãƒ‰ã‚¦ã‚µã‚¤ã‚º", 2, 10, 5)
                    min_count = st.sidebar.slider("æœ€å°å‡ºç¾å›æ•°", 1, 10, 2)
                    top_n = st.sidebar.slider("è¡¨ç¤ºã™ã‚‹å…±èµ·ãƒšã‚¢æ•°", 10, 50, 30)
                    
                    cooccurrence, word_count = analyzer.cooccurrence_network(
                        text, 
                        window_size=window_size, 
                        min_count=min_count
                    )
                    
                    if cooccurrence:
                        # å…±èµ·é »åº¦ãƒˆãƒƒãƒ—10
                        st.subheader("å…±èµ·é »åº¦ãƒˆãƒƒãƒ—10")
                        top_cooccurrence = sorted(cooccurrence.items(), key=lambda x: x[1], reverse=True)[:10]
                        df_cooccurrence = pd.DataFrame(
                            [(f"{pair[0]} - {pair[1]}", count) for pair, count in top_cooccurrence],
                            columns=['å˜èªãƒšã‚¢', 'å…±èµ·å›æ•°']
                        )
                        st.dataframe(df_cooccurrence, use_container_width=True)
                        
                        # ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã‚°ãƒ©ãƒ•
                        st.subheader("å…±èµ·ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã‚°ãƒ©ãƒ•")
                        network_fig = analyzer.create_network_graph(cooccurrence, word_count, top_n=top_n)
                        if network_fig:
                            st.plotly_chart(network_fig, use_container_width=True)
                        else:
                            st.warning("ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã‚°ãƒ©ãƒ•ã‚’ç”Ÿæˆã§ãã¾ã›ã‚“ã§ã—ãŸã€‚")
                    else:
                        st.warning("å…±èµ·ãƒ‡ãƒ¼ã‚¿ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’èª¿æ•´ã—ã¦ãã ã•ã„ã€‚")
                    
                    st.success("âœ… åˆ†æãŒå®Œäº†ã—ã¾ã—ãŸï¼")
        
        except Exception as e:
            st.error(f"ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {str(e)}")
            import traceback
            st.code(traceback.format_exc())
    
    else:
        st.info("ğŸ‘ˆ ã‚µã‚¤ãƒ‰ãƒãƒ¼ã‹ã‚‰ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã—ã¦ãã ã•ã„")
        
        # ã‚µãƒ³ãƒ—ãƒ«ãƒ†ã‚­ã‚¹ãƒˆã§è©¦ã™
        st.markdown("---")
        st.subheader("ã‚µãƒ³ãƒ—ãƒ«ãƒ†ã‚­ã‚¹ãƒˆã§è©¦ã™")
        
        sample_text = st.text_area(
            "ãƒ†ã‚­ã‚¹ãƒˆã‚’å…¥åŠ›ã—ã¦ãã ã•ã„",
            "ä»Šæ—¥ã¯æœ¬å½“ã«ç´ æ™´ã‚‰ã—ã„å¤©æ°—ã§ã€å…¬åœ’ã§æ¥½ã—ãéŠã³ã¾ã—ãŸã€‚å­ä¾›ãŸã¡ã‚‚å¤§å–œã³ã§ã€ã¨ã¦ã‚‚å¹¸ã›ãªä¸€æ—¥ã§ã—ãŸã€‚"
            "ã—ã‹ã—ã€å¸°ã‚Šé“ã§å°‘ã—ç–²ã‚Œã¦ã—ã¾ã„ã€å°‘ã—æ®‹å¿µãªæ°—æŒã¡ã«ãªã‚Šã¾ã—ãŸã€‚",
            height=150
        )
        
        if st.button("ã‚µãƒ³ãƒ—ãƒ«ãƒ†ã‚­ã‚¹ãƒˆã‚’åˆ†æ", type="secondary"):
            with st.spinner("åˆ†æä¸­..."):
                tokens = analyzer.tokenize(sample_text, pos_filter=['åè©', 'å‹•è©', 'å½¢å®¹è©', 'å‰¯è©'])
                
                st.subheader("å½¢æ…‹ç´ è§£æçµæœ")
                df_tokens = pd.DataFrame(tokens)
                st.dataframe(df_tokens.head(20), use_container_width=True)
                
                st.subheader("ã‚»ãƒ³ãƒãƒ¡ãƒ³ãƒˆåˆ†æçµæœ")
                sentiment_result = analyzer.sentiment_analysis(tokens)
                col1, col2, col3 = st.columns(3)
                with col1:
                    st.metric("ãƒã‚¸ãƒ†ã‚£ãƒ–", sentiment_result['positive'])
                with col2:
                    st.metric("ãƒã‚¬ãƒ†ã‚£ãƒ–", sentiment_result['negative'])
                with col3:
                    st.metric("ã‚¹ã‚³ã‚¢", f"{sentiment_result['score']:.3f}")


if __name__ == "__main__":
    main()
